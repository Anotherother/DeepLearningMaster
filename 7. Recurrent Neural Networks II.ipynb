{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN in ``tf`` \n",
    "\n",
    "LSTM model in ``tf``:\n",
    "\n",
    "<center>\n",
    "<img src=\"images/tf-lstm.png\" alt=\"\" style=\"width: 600px;\"/> \n",
    "Source: https://cloud.google.com/blog/big-data/2017/01/learn-tensorflow-and-deep-learning-without-a-phd\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU model in ``tf``:\n",
    "\n",
    "<center>\n",
    "<img src=\"images/tf-gru.png\" alt=\"\" style=\"width: 600px;\"/> \n",
    "Source: https://cloud.google.com/blog/big-data/2017/01/learn-tensorflow-and-deep-learning-without-a-phd\n",
    "<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning to add in ``tf``\n",
    "\n",
    "Source: http://projects.rajivshah.com/blog/2016/04/05/rnn_addition/ \n",
    "\n",
    "The objective of this code developed by Rajiv Shah is to train a RNN for adding a sequence of integers. In this acse we must process all the sequence in order to produce a result. We will use the ``tf`` implementation of the ``seq2seq`` architecure, originally proposed by Sutskever, Vinyals and Le in 2014. \n",
    "\n",
    "The architecture diagram from their paper is:\n",
    "\n",
    "<center>\n",
    "<img src=\"images/seq2seq.png\" alt=\"\" style=\"width: 800px;\"/> \n",
    "Source: https://arxiv.org/abs/1409.3215\n",
    "<center>\n",
    "\n",
    "Rectangles are recurrent layers. Encoder receives ``[A, B, C]`` sequence as inputs. We don't care about encoder outputs, only about the hidden state it accumulates while reading the sequence. \n",
    "\n",
    "After input sequence ends, encoder passes its final state to decoder, which receives ``[<EOS>, W, X, Y, Z]`` and is trained to output ``[W, X, Y, Z, <EOS>]``. \n",
    "\n",
    "``tf`` implementation of the ``seq2seq`` allows sequences of different lenghts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import basic libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn_cell\n",
    "from tensorflow.python.ops import rnn\n",
    "from tensorflow.python.ops import seq2seq\n",
    "from numpy import sum\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import *\n",
    "%matplotlib inline  \n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define first a set of hyperparameters, being the most important ``num_units``, that is the parameter that represents the internal memory in the basic LSTM cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_units = 50   # units in LSTM cell\n",
    "input_size = 1   # input dimension\n",
    "batch_size = 50    \n",
    "seq_len = 7      # sequence lenght\n",
    "drop_out = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can write an auxiliar function to generate random sequences of integers (and the result of their addition):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[[1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]]]), array([ 1.]))\n"
     ]
    }
   ],
   "source": [
    "# Creates our random sequences\n",
    "def gen_data(min_length=5, max_length=15, n_batch=50):\n",
    "    \n",
    "    X = np.concatenate([np.random.randint(10,size=(n_batch, max_length, 1))],\n",
    "                       axis=-1)\n",
    "    y = np.zeros((n_batch,))\n",
    "    # Compute masks and correct values\n",
    "    for n in range(n_batch):\n",
    "        # Randomly choose the sequence length\n",
    "        length = np.random.randint(min_length, max_length)\n",
    "        X[n, length:, 0] = 0\n",
    "        # Sum the dimensions of X to get the target value\n",
    "        y[n] = np.sum(X[n, :, 0]*1)\n",
    "    return (X,y)\n",
    "\n",
    "print gen_data(2,seq_len,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to start the model construction phase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "# Model architecture\n",
    "\n",
    "num_layers = 2\n",
    "cell = rnn_cell.BasicLSTMCell(num_units)\n",
    "cell = rnn_cell.MultiRNNCell([cell] * num_layers)\n",
    "cell = rnn_cell.DropoutWrapper(cell,output_keep_prob=drop_out)\n",
    "\n",
    "# Create placeholders for X and y\n",
    "\n",
    "inputs = [tf.placeholder(tf.float32,shape=[batch_size,1]) \n",
    "          for _ in range(seq_len)]\n",
    "result = tf.placeholder(tf.float32, shape=[batch_size])\n",
    "\n",
    "# We initialize the initial cell state to 0\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "# We use a rnn_decoder model\n",
    "# outputs: A list of the same length as decoder_inputs \n",
    "# of 2D Tensors with shape [batch_size x input_size] containing \n",
    "# generated outputs\n",
    "outputs, states = seq2seq.rnn_decoder(inputs, \n",
    "                                      initial_state, \n",
    "                                      cell, \n",
    "                                      scope ='rnnln')\n",
    "\n",
    "# We are only interested in the final decoder output value\n",
    "outputs2 = outputs[-1]\n",
    "\n",
    "# Tranformation of the final LSTM output value to a real value\n",
    "\n",
    "W_o = tf.Variable(tf.random_normal([num_units,input_size], stddev=0.01))     \n",
    "b_o = tf.Variable(tf.random_normal([input_size], stddev=0.01))\n",
    "outputs3 = tf.matmul(outputs2, W_o) + b_o\n",
    "\n",
    "# Definition of the mean square loss function\n",
    "\n",
    "cost = tf.pow(tf.sub(tf.reshape(outputs3, [-1]), result),2)\n",
    "train_op = tf.train.RMSPropOptimizer(0.005, 0.2).minimize(cost) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Generate Validation Data\n",
    "\n",
    "tempX,y_val = gen_data(6,seq_len,batch_size)\n",
    "X_val = []\n",
    "for i in range(seq_len):\n",
    "    X_val.append(tempX[:,i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Session\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "train_score =[]\n",
    "val_score= []\n",
    "x_axis=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [01:32<00:00, 108.31it/s]\n"
     ]
    }
   ],
   "source": [
    "num_epochs=10000\n",
    " \n",
    "for k in tqdm(range(1,num_epochs)):\n",
    "\n",
    "    #Generate Data for each epoch\n",
    "    tempX,y = gen_data(5,seq_len,batch_size)\n",
    "    X = []\n",
    "    for i in range(seq_len):\n",
    "        X.append(tempX[:,i,:])\n",
    "\n",
    "    #Create the dictionary of inputs to feed into sess.run\n",
    "    temp_dict = {inputs[i]:X[i] for i in range(seq_len)}\n",
    "    temp_dict.update({result: y})\n",
    "\n",
    "    _,c_train = sess.run([train_op,cost],feed_dict=temp_dict)   \n",
    "    #perform an update on the parameters\n",
    "\n",
    "    val_dict = {inputs[i]:X_val[i] for i in range(seq_len)}  \n",
    "    #create validation dictionary\n",
    "    \n",
    "    val_dict.update({result: y_val})\n",
    "    c_val = sess.run([cost],feed_dict = val_dict )            \n",
    "    #compute the cost on the validation set\n",
    "    \n",
    "    if (k%100==0):\n",
    "        train_score.append(sum(c_train))\n",
    "        val_score.append(sum(c_val))\n",
    "        x_axis.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Train cost: 180.053588867, on Epoch 9999\n",
      "Final Validation cost: 422.151092529, on Epoch 9999\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhoAAAFkCAYAAABmeZIKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMTQEQRAEQYgZYmgIgiAIghAz/h8Bi/CdpB5IoAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4099609e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print \"Final Train cost: {}, on Epoch {}\".format(train_score[-1],k)\n",
    "print \"Final Validation cost: {}, on Epoch {}\".format(val_score[-1],k)\n",
    "plt.plot(train_score, 'r-', val_score, 'b-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##This part generates a new validation set to test against\n",
    "val_score_v =[]\n",
    "num_epochs=1\n",
    "\n",
    "for k in range(num_epochs):\n",
    "\n",
    "    #Generate Data for each epoch\n",
    "    tempX,y = gen_data(5,seq_len,batch_size)\n",
    "    X = []\n",
    "    for i in range(seq_len):\n",
    "        X.append(tempX[:,i,:])\n",
    "\n",
    "    val_dict = {inputs[i]:X[i] for i in range(seq_len)}\n",
    "    val_dict.update({result: y})\n",
    "    outv, c_val = sess.run([outputs3,cost],feed_dict = val_dict ) \n",
    "    val_score_v.append([c_val])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2],\n",
       "        [9],\n",
       "        [5],\n",
       "        [5],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]]), 21.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Target\n",
    "tempX[3],y[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 20.11400604], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prediction\n",
    "outv[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\n",
    "\n",
    "This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\n",
    "Long Short Term Memory paper: http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf\n",
    "\n",
    "Author: Aymeric Damien\n",
    "Project: https://github.com/aymericdamien/TensorFlow-Examples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "import numpy as np\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify images using a recurrent neural network, we consider every image row as a sequence of pixels. \n",
    "\n",
    "Because MNIST image shape is $28 \\times 28$ px, we will then handle 28 sequences of 28 steps for every sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate  = 0.001\n",
    "training_iters = 100000\n",
    "batch_size     = 128\n",
    "\n",
    "display_step   = 50\n",
    "\n",
    "# Network Parameters\n",
    "n_input        = 28 # number of sequences for every sample\n",
    "n_steps        = 28 # number of timesteps for every sequence\n",
    "n_hidden       = 64 # hidden layer num of features\n",
    "n_classes      = 10 # total classes (0-9 digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
    "\n",
    "\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_input, n_hidden])), \n",
    "    # Hidden layer weights\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_hidden])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "    \n",
    "    # Permuting batch_size and n_steps\n",
    "    x = tf.transpose(x, [1, 0, 2])\n",
    "    # Reshaping to (n_steps*batch_size, n_input)\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "    # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.split(0, n_steps, x)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "pred = RNN(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 6400, Minibatch Loss= 1.102225, Training Accuracy= 0.62500\n",
      "Iter 12800, Minibatch Loss= 1.113379, Training Accuracy= 0.59375\n",
      "Iter 19200, Minibatch Loss= 0.586293, Training Accuracy= 0.83594\n",
      "Iter 25600, Minibatch Loss= 0.593785, Training Accuracy= 0.81250\n",
      "Iter 32000, Minibatch Loss= 0.288821, Training Accuracy= 0.92969\n",
      "Iter 38400, Minibatch Loss= 0.175552, Training Accuracy= 0.93750\n",
      "Iter 44800, Minibatch Loss= 0.262320, Training Accuracy= 0.91406\n",
      "Iter 51200, Minibatch Loss= 0.203024, Training Accuracy= 0.92969\n",
      "Iter 57600, Minibatch Loss= 0.201458, Training Accuracy= 0.94531\n",
      "Iter 64000, Minibatch Loss= 0.217671, Training Accuracy= 0.92188\n",
      "Iter 70400, Minibatch Loss= 0.189884, Training Accuracy= 0.94531\n",
      "Iter 76800, Minibatch Loss= 0.078310, Training Accuracy= 0.97656\n",
      "Iter 83200, Minibatch Loss= 0.201639, Training Accuracy= 0.92969\n",
      "Iter 89600, Minibatch Loss= 0.182728, Training Accuracy= 0.94531\n",
      "Iter 96000, Minibatch Loss= 0.242476, Training Accuracy= 0.91406\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.945312\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Reshape data to get 28 seq of 28 elements\n",
    "        batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n",
    "            # Calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "            print \"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc)\n",
    "        step += 1\n",
    "    print \"Optimization Finished!\"\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "    test_len = 128\n",
    "    test_data = mnist.test.images[:test_len].reshape((-1, n_steps, n_input))\n",
    "    test_label = mnist.test.labels[:test_len]\n",
    "    print \"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_data, y: test_label})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM\n",
    "\n",
    "So  far,  we  have  focused  on  RNNs  that  look  into  the  past    to predict future values in the sequence, but not to  to make predictions based on future values by reading throught the sequence backwards?\n",
    "\n",
    "**Bi-directional  deep  neural networs**,  at each time-step, $t$,  maintain two hidden layers, one for the left-to-right propagation and another for the right-to-left  propagation (hence, consuming twice  as  much  memory  space).   \n",
    "\n",
    "The final classification result, $\\hat{y}$, is generated through combining the score results produced by both RNN hidden layers.\n",
    "\n",
    "<img src = \"./images/t9.png\"  width = \"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/\", one_hot=True)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 100000\n",
    "batch_size = 128\n",
    "display_step = 100\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 28 # MNIST data input (img shape: 28*28)\n",
    "n_steps = 28 # timesteps\n",
    "n_hidden = 128 # hidden layer num of features\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    # Hidden layer weights => 2*n_hidden because of foward + backward cells\n",
    "    'out': tf.Variable(tf.random_normal([2*n_hidden, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def BiRNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `bidirectional_rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "    \n",
    "    # Permuting batch_size and n_steps\n",
    "    x = tf.transpose(x, [1, 0, 2])\n",
    "    # Reshape to (n_steps*batch_size, n_input)\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "    # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.split(0, n_steps, x)\n",
    "\n",
    "    # Define lstm cells with tensorflow\n",
    "    # Forward direction cell\n",
    "    lstm_fw_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "    # Backward direction cell\n",
    "    lstm_bw_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, _, _ = rnn.bidirectional_rnn(lstm_fw_cell, \n",
    "                                          lstm_bw_cell, \n",
    "                                          x,\n",
    "                                          dtype=tf.float32)\n",
    "    \n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 12800, Minibatch Loss= 0.613071, Training Accuracy= 0.82031\n",
      "Iter 25600, Minibatch Loss= 0.336384, Training Accuracy= 0.89844\n",
      "Iter 38400, Minibatch Loss= 0.142558, Training Accuracy= 0.96875\n",
      "Iter 51200, Minibatch Loss= 0.133584, Training Accuracy= 0.93750\n",
      "Iter 64000, Minibatch Loss= 0.201798, Training Accuracy= 0.94531\n",
      "Iter 76800, Minibatch Loss= 0.109608, Training Accuracy= 0.97656\n",
      "Iter 89600, Minibatch Loss= 0.101680, Training Accuracy= 0.96094\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.976562\n"
     ]
    }
   ],
   "source": [
    "pred = BiRNN(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Reshape data to get 28 seq of 28 elements\n",
    "        batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n",
    "            # Calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "            print \"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc)\n",
    "        step += 1\n",
    "    print \"Optimization Finished!\"\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "    test_len = 128\n",
    "    test_data = mnist.test.images[:test_len].reshape((-1, n_steps, n_input))\n",
    "    test_label = mnist.test.labels[:test_len]\n",
    "    print \"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_data, y: test_label})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name generation with LSTM\n",
    "\n",
    "We are going to train RNN \"character-level\" language models. \n",
    "\n",
    "That is, we’ll give the RNN a huge chunk of text and ask it to model the probability distribution of the next character in the sequence given a sequence of previous characters. This will then allow us to generate new text one character at a time.\n",
    "\n",
    "We will encode each character into a vector using ``1-of-k`` encoding (i.e. all zero except for a single one at the index of the character in the vocabulary), and feed them into the RNN one at a time. \n",
    "\n",
    "At test time, we will feed a character into the RNN and get a distribution over what characters are likely to come next. We sample from this distribution, and feed it right back in to get the next letter. Repeat this process and you’re sampling text!\n",
    "\n",
    "We can also play with the temperature of the Softmax during sampling. Decreasing the temperature from 1 to some lower number (e.g. 0.5) makes the RNN more confident, but also more conservative in its samples. Conversely, higher temperatures will give more diversity but at cost of more mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to process sequences of symbols with RNN we need to represent these symbols by numbers.\n",
    "\n",
    "Let's suppose we have $|V|$ different symbols. The most simple representation is the **one-hot vector**: Represent every word as an $\\mathbb{R}^{|V|\\times1}$ vector with all $0$s and one $1$ at the index of that word in the sorted english language. Symbol vectors in this type of encoding would appear as the following:\n",
    "\n",
    "$$w^{s_1} = \\left[ \\begin{array}{c} 1 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{array} \\right], w^{s_2} = \\left[ \\begin{array}{c} 0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{array} \\right], w^{s_3} = \\left[ \\begin{array}{c} 0 \\\\ 0 \\\\ 1 \\\\ \\vdots \\\\ 0 \\end{array} \\right], \\cdots \n",
    "w^{s_{|V|}} = \\left[ \\begin{array}{c} 0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\end{array} \\right] $$\n",
    "\n",
    "We represent each symbol as a completely independent entity. This symbol representation does not give us directly any notion of similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model we need text to learn from a large dataset of names. Fortunately we don’t need any labels to train a language model, just raw text.\n",
    "\n",
    "Places names: You can download 52,700 Catalan names from a dataset available on http://territori.gencat.cat/ca/01_departament/11_normativa_i_documentacio/03_documentacio/02_territori_i_mobilitat/cartografia/nomenclator_oficial_de_toponimia_de_catalunya/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 47527\n",
      "total chars: 30\n",
      "nb sequences: 15836\n",
      "Vectorization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import codecs\n",
    "f = codecs.open('data/NombresMujerBarcelona.txt', \"r\", \"utf-8\")\n",
    "#f = codecs.open('data/toponims.txt', \"r\", \"utf-8\")\n",
    "string = f.read()\n",
    "string.encode('utf-8')\n",
    "text = string.lower()\n",
    "\n",
    "# text = text.replace(\"\\n\", \" \")\n",
    "    \n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 20\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, \n",
    "#               return_sequences=True,\n",
    "               dropout_W=0.2, \n",
    "               dropout_U=0.2, \n",
    "               input_shape=(maxlen, len(chars))))\n",
    "#model.add(LSTM(64, \n",
    "#               dropout_W=0.2, \n",
    "#               dropout_U=0.2))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to use the Keras LSTM model to make predictions is to first start off with a seed sequence as input, generate the next character then update the seed sequence to add the generated character on the end and trim off the first character. \n",
    "\n",
    "This process is repeated for as long as we want to predict new characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Epoch 1/1\n",
      "15836/15836 [==============================] - 10s - loss: 2.9779    \n",
      "----- Generating with seed: \"nia drifa drissia du\"\n",
      "\n",
      "----- diversity: 0.5\n",
      "lzrene eiarialiavieiqiblmariarieriayiaalariantanlr\n",
      "\n",
      "----- diversity: 1.0\n",
      "icomgemesstçilbbgnrçaxntrv\n",
      "unaafiktnaçkengacmisndi\n",
      "\n",
      "----- diversity: 1.2\n",
      "htliyclqegerlñflil ñcrmenepynf\n",
      "seiavqwuwiyiqkan vj\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 2\n",
      "Epoch 1/1\n",
      "15836/15836 [==============================] - 9s - loss: 2.7471     \n",
      "----- Generating with seed: \" maria estel maria e\"\n",
      "\n",
      "----- diversity: 0.5\n",
      "lelin\n",
      "avianza\n",
      "maria m riti\n",
      "lfli\n",
      "nanila\n",
      "maria\n",
      "veeli\n",
      "\n",
      "----- diversity: 1.0\n",
      "ltczaliw\n",
      "maciura\n",
      "cesvaxisgelcbenyçwaemenvauil\n",
      "gieç\n",
      "\n",
      "----- diversity: 1.2\n",
      "ç\n",
      "gewlavixercfbizariayttliuxzoifhkeqibbe\n",
      "evelgmvri\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 3\n",
      "Epoch 1/1\n",
      "15836/15836 [==============================] - 10s - loss: 2.7807    \n",
      "----- Generating with seed: \"ina sara elena sara \"\n",
      "\n",
      "----- diversity: 0.5\n",
      "pjxzgsdplñb q\n",
      "b\n",
      "hcbnzpquyxgñbaadherl nçjixnkcssçzm\n",
      "\n",
      "----- diversity: 1.0\n",
      " vfm dlurqdlfidmyñqdfdaogoghñg lxabhdnbxsbqndhçglq\n",
      "\n",
      "----- diversity: 1.2\n",
      "tqmekfvajpñpegzofupoozvy\n",
      "rpçñrkefhoyozlhkgñrzj\n",
      "wvo\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 4\n",
      "Epoch 1/1\n",
      "15836/15836 [==============================] - 9s - loss: 3.3629     \n",
      "----- Generating with seed: \"huizhen huma humaira\"\n",
      "\n",
      "----- diversity: 0.5\n",
      " eg narñxa ugfñuxhqa xinua \n",
      "nysomxrpsfnjmryzghgqij\n",
      "\n",
      "----- diversity: 1.0\n",
      "geçzmkgbsjhkxhha g\n",
      "a netudperhsnjbddvsjiylz mhpoww\n",
      "\n",
      "----- diversity: 1.2\n",
      "dçuççhog\n",
      "evsu cyçcgqqwhfiwastkljxxxrokneakswmcrjpx\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 5\n",
      "Epoch 1/1\n",
      "15836/15836 [==============================] - 9s - loss: 2.7467     \n",
      "----- Generating with seed: \"nazaret maria nela m\"\n",
      "\n",
      "----- diversity: 0.5\n",
      "aria\n",
      "aria\n",
      "anaria\n",
      "afza elyra\n",
      "meriaxahela\n",
      "maria\n",
      "anes\n",
      "\n",
      "----- diversity: 1.0\n",
      "marg kdrr\n",
      "nnarcina\n",
      "pfpraña\n",
      "anfen\n",
      "naomakjena\n",
      "maz\n",
      "añ\n",
      "\n",
      "----- diversity: 1.2\n",
      "xo\n",
      "fes\n",
      "bdegñajçjxitih\n",
      "ygwdmekleraeiziav\n",
      "hfrsp\n",
      "irua\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 6\n",
      "Epoch 1/1\n",
      "15836/15836 [==============================] - 11s - loss: 2.5677    \n",
      "----- Generating with seed: \"a andrea alejandra b\"\n",
      "\n",
      "----- diversity: 0.5\n",
      "enaria\n",
      "maria elana\n",
      "maria alia\n",
      "anjana\n",
      "naziana\n",
      "alia\n",
      "\n",
      "\n",
      "----- diversity: 1.0\n",
      "alñçyasina\n",
      "sileqinduñoiniçwtxanonjea\n",
      "wirbia\n",
      "tetyja\n",
      "\n",
      "----- diversity: 1.2\n",
      "\n",
      "jjenevçjfanha\n",
      "rwfmia\n",
      "\n",
      "pmagintunynteanaka wxeoieaz\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 7\n",
      "Epoch 1/1\n",
      "15836/15836 [==============================] - 12s - loss: 2.5366    \n",
      "----- Generating with seed: \"ella anacleta anahi \"\n",
      "\n",
      "----- diversity: 0.5\n",
      "maria dantina\n",
      "maria kanya\n",
      "maria ola\n",
      "maria maria el\n",
      "\n",
      "----- diversity: 1.0\n",
      "tebha\n",
      "pwasbelits\n",
      "qbeupa etriqa\n",
      " gareruça\n",
      "ebaeraume\n",
      "\n",
      "----- diversity: 1.2\n",
      "ycidse\n",
      "stm hakp  outimnez\n",
      "maria loweugjinanabpuldf\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 8\n",
      "Epoch 1/1\n",
      "15836/15836 [==============================] - 11s - loss: 2.4955    \n",
      "----- Generating with seed: \"adiya nadya nagore n\"\n",
      "\n",
      "----- diversity: 0.5\n",
      "ava\n",
      "amaria\n",
      "eaelen\n",
      "maria belia\n",
      "urana   arue\n",
      "ara rap\n",
      "\n",
      "----- diversity: 1.0\n",
      "rela açyça aldra\n",
      "maria çasa\n",
      "na\n",
      "uwwa\n",
      "mzlex marmja\n",
      "r\n",
      "\n",
      "----- diversity: 1.2\n",
      "aka kaz\n",
      "qarmeg\n",
      "txçnfrna\n",
      "varnaysbabeqa\n",
      "maruçvnañgwe\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 9\n",
      "Epoch 1/1\n",
      "15836/15836 [==============================] - 11s - loss: 2.4464    \n",
      "----- Generating with seed: \"a maria gloria maria\"\n",
      "\n",
      "----- diversity: 0.5\n",
      " maria\n",
      "maria glqia\n",
      "taria maria\n",
      "gelina alena\n",
      "saria \n",
      "\n",
      "----- diversity: 1.0\n",
      "eaçlinl\n",
      "\n",
      "\n",
      "arza  avoroy\n",
      "e rz afqdmacenjg\n",
      "payma\n",
      "aorx\n",
      "\n",
      "----- diversity: 1.2\n",
      "vdrza\n",
      "kageiqrejw\n",
      "alnej\n",
      "alyto\n",
      "zxicat\n",
      "meuma a\n",
      "jxinck\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 10\n",
      "Epoch 1/1\n",
      "15836/15836 [==============================] - 10s - loss: 2.4025    \n",
      "----- Generating with seed: \" lu lua luana lubna \"\n",
      "\n",
      "----- diversity: 0.5\n",
      "ftanipu\n",
      "isa\n",
      "gera\n",
      "jansia\n",
      "rina\n",
      "a aria\n",
      "ana iida\n",
      "anxa \n",
      "\n",
      "----- diversity: 1.0\n",
      "maria\n",
      "anka  tañ manva\n",
      "mawlvia ñtsx ñçasusra\n",
      "çqbce\n",
      "\n",
      "\n",
      "----- diversity: 1.2\n",
      "novfra\n",
      "ñtfar\n",
      "ñjdja\n",
      "pj\n",
      "vsmçyccña\n",
      "yesay\n",
      "j f bwigzave\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 11\n",
      "Epoch 1/1\n",
      "15836/15836 [==============================] - 10s - loss: 2.3999    \n",
      "----- Generating with seed: \"ia susana silvia ter\"\n",
      "\n",
      "----- diversity: 0.5\n",
      "is\n",
      "ñina maria\n",
      "lilexa maria\n",
      "maria aliwa\n",
      "maria alele\n",
      "\n",
      "----- diversity: 1.0\n",
      "y\n",
      "zesa albena\n",
      "garvxajiq\n",
      "yayngina\n",
      "waostig\n",
      "sabañahsp\n",
      "\n",
      "----- diversity: 1.2\n",
      "qexanwqh\n",
      "anampaih\n",
      "ahagisaifaxawrçzialajçshñaggalvi\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 12\n",
      "Epoch 1/1\n",
      "15836/15836 [==============================] - 10s - loss: 2.3658    \n",
      "----- Generating with seed: \"zenaida zhang zhanna\"\n",
      "\n",
      "----- diversity: 0.5\n",
      "\n",
      "nina\n",
      "nina\n",
      "niana\n",
      "riwabta\n",
      "jixelana\n",
      "kwina\n",
      "cilisbe\n",
      "\n",
      "l\n",
      "\n",
      "----- diversity: 1.0\n",
      "gñwbkacman\n",
      "yiswarc\n",
      "cbepçlar elrbew\n",
      "xiqna rxankñ\n",
      "in\n",
      "\n",
      "----- diversity: 1.2\n",
      "\n",
      "çjpswdbelzlczgyjncenpa\n",
      "irhppssa\n",
      "qaifjesa\n",
      "thflh\n",
      "wa\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 13\n",
      "Epoch 1/1\n",
      "15836/15836 [==============================] - 11s - loss: 2.3377    \n",
      "----- Generating with seed: \"ela daniela alejandr\"\n",
      "\n",
      "----- diversity: 0.5\n",
      "a\n",
      "ana ñanina\n",
      "ana yanna ela\n",
      "ana uanesa\n",
      "ana janena\n",
      "a\n",
      "\n",
      "----- diversity: 1.0\n",
      "nsa emanelñta yabça tsbea\n",
      "ç\n",
      "a ykodiqjlba karia\n",
      "isa\n",
      "\n",
      "----- diversity: 1.2\n",
      "nxiniiqam dk elina\n",
      "lvcakfaea\n",
      "umiqna\n",
      "lserjel\n",
      "crijdq\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 14\n",
      "Epoch 1/1\n",
      "15836/15836 [==============================] - 10s - loss: 2.3162    \n",
      "----- Generating with seed: \"liana maria juliane \"\n",
      "\n",
      "----- diversity: 0.5\n",
      "sialina\n",
      "silia\n",
      "litpia\n",
      "hissa\n",
      "lizpa\n",
      "lia\n",
      "lia\n",
      "liya\n",
      "fila\n",
      "\n",
      "----- diversity: 1.0\n",
      "\n",
      "alariyns aleo\n",
      "\n",
      "acia maehna\n",
      "zavmija yahjana\n",
      "e\n",
      "anse\n",
      "\n",
      "----- diversity: 1.2\n",
      "ss\n",
      "waoqbaq\n",
      "rampm vaculitmw\n",
      "mariwqsqnjwçihedaa\n",
      "mkki\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 15\n",
      "Epoch 1/1\n",
      "15836/15836 [==============================] - 10s - loss: 2.2966    \n",
      "----- Generating with seed: \"ha samar samara sama\"\n",
      "\n",
      "----- diversity: 0.5\n",
      "f\n",
      "salia\n",
      "raba\n",
      "haima\n",
      "samel\n",
      "uamina\n",
      "safna\n",
      "bama\n",
      "zaviba\n",
      "\n",
      "\n",
      "----- diversity: 1.0\n",
      "qjg ypa\n",
      "xiaesa\n",
      "pñzxha\n",
      "sheuna\n",
      "lxqmelilcde\n",
      "eril aara\n",
      "\n",
      "----- diversity: 1.2\n",
      "ña\n",
      "arih onbjeu\n",
      "harz t euxtça\n",
      "maria \n",
      "qenanaez\n",
      "pnrci\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 16\n",
      "Epoch 1/1\n",
      "15836/15836 [==============================] - 9s - loss: 2.2876     \n",
      "----- Generating with seed: \"ntoinette antolina a\"\n",
      "\n",
      "----- diversity: 0.5\n",
      "navilina\n",
      "anawana\n",
      "ana maria\n",
      "ana wina\n",
      "ana maria\n",
      "ana \n",
      "\n",
      "----- diversity: 1.0\n",
      "lkw\n",
      "anciwa  a\n",
      "pabpiya\n",
      "aita  maria\n",
      "anqla isay phorb\n",
      "\n",
      "----- diversity: 1.2\n",
      "a\n",
      "ana mranen\n",
      "gariña qyafjtvlau\n",
      " anscika\n",
      " hdafia\n",
      "ng\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 17\n",
      "Epoch 1/1\n",
      "15836/15836 [==============================] - 10s - loss: 2.2660    \n",
      "----- Generating with seed: \"milagros maria milag\"\n",
      "\n",
      "----- diversity: 0.5\n",
      "saila\n",
      "ldia\n",
      "lia\n",
      "lianda\n",
      "lxiana\n",
      "lira\n",
      "sisma\n",
      "liza\n",
      "liva\n",
      "\n",
      "\n",
      "----- diversity: 1.0\n",
      "gilgñzñ\n",
      "divpdh dakiag\n",
      "clprfer qizulus\n",
      "alwlma\n",
      "ala\n",
      "a\n",
      "\n",
      "----- diversity: 1.2\n",
      "gniwa\n",
      "axa elezla\n",
      "zanatisea\n",
      "yjahfilce maranjanorihj\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 18\n",
      "Epoch 1/1\n",
      "15836/15836 [==============================] - 10s - loss: 2.2383    \n",
      "----- Generating with seed: \"fattouch fattouma fa\"\n",
      "\n",
      "----- diversity: 0.5\n",
      "jaya\n",
      "amalawa\n",
      "amaopa\n",
      "assaja\n",
      "aagelene\n",
      "ara maria\n",
      "arao\n",
      "\n",
      "----- diversity: 1.0\n",
      "bewfa\n",
      "anqi\n",
      "aflaiia\n",
      "aona calwafa\n",
      "anfa isak\n",
      "narxa em\n",
      "\n",
      "----- diversity: 1.2\n",
      "kaqmaria  mariwpusrbema\n",
      "elizauqelfzlez\n",
      "z\n",
      "iwefñe\n",
      "t \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 19\n",
      "Epoch 1/1\n",
      " 1792/15836 [==>...........................] - ETA: 10s - loss: 2.1683"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python2.7/inspect.py\", line 1044, in getinnerframes\n",
      "    framelist.append((tb.tb_frame,) + getframeinfo(tb, context))\n",
      "  File \"/usr/lib/python2.7/inspect.py\", line 1004, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python2.7/inspect.py\", line 454, in getsourcefile\n",
      "    if hasattr(getmodule(object, filename), '__loader__'):\n",
      "  File \"/usr/lib/python2.7/inspect.py\", line 493, in getmodule\n",
      "    if f == _filesbymodname.get(modname, None):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2897\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2900\u001b[0m             \u001b[0moutflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only)\u001b[0m\n\u001b[1;32m   1822\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1823\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0;32m-> 1824\u001b[0;31m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[1;32m   1825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1826\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1404\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[0;32m-> 1406\u001b[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[1;32m   1407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[0;32m-> 1314\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m             )\n\u001b[1;32m   1316\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1196\u001b[0m                 \u001b[0mstructured_traceback_parts\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mformatted_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m             \u001b[0mstructured_traceback_parts\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mformatted_exception\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstructured_traceback_parts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "# train the model, output generated text after each iteration\n",
    "for iteration in range(1, 60):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(X, y, batch_size=256, nb_epoch=1)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    print('----- Generating with seed: \"' + sentence.replace(\"\\n\", \" \") + '\"')\n",
    "        \n",
    "    for diversity in [0.5, 1.0, 1.2]:\n",
    "        print()\n",
    "        print('----- diversity:', diversity)\n",
    "        for i in range(50):\n",
    "            \n",
    "            x = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'lianda' in text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In the case of places, after several hours you can generate names such as:\n",
    "\n",
    "+ Alzinetes, torrent de les\n",
    "+ Alzinetes, vall de les\n",
    "+ **Alzinó, Mas d'**\n",
    "+ Alzinosa, collada de l'\n",
    "+ Alzinosa, font de l'\n",
    "\n",
    "-\n",
    "\n",
    "+ Benavent, roc de\n",
    "+ Benaviure, Cal\n",
    "+ **Benca**\n",
    "+ Bendiners, pla de\n",
    "+ Benedi, roc del\n",
    "\n",
    "-\n",
    "\n",
    "+ Fiola, la\n",
    "+ Fiola, puig de la\n",
    "+ **Fiper, Granja del**\n",
    "+ Firassa, Finca\n",
    "+ Firell\n",
    "\n",
    "-\n",
    "\n",
    "+ Regueret, lo\n",
    "+ Regueret, lo\n",
    "+ **Regueró**\n",
    "+ Reguerols, els\n",
    "+ Reguerons, els\n",
    "\n",
    "-\n",
    "\n",
    "+ Vallverdú, Mas de\n",
    "+ Vallverdú, serrat de\n",
    "+ **Vallvicamanyà**\n",
    "+ Vallvidrera\n",
    "+ Vallvidrera, riera de\n",
    "\n",
    "-\n",
    "\n",
    "+ Terraubella, Corral de\n",
    "+ Terraubes\n",
    "+ **Terravanca**\n",
    "+ Terrer Nou, Can\n",
    "+ Terrer Roig, lo\n",
    "\n",
    "where names in **bold** are generated and other names are the nearest neighbours (in the training dataset) of the generated name."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
